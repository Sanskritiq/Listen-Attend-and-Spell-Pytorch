{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------Processing Datasets----------\n",
      "Training sets : train-clean-100/\n",
      "Validation sets : dev-clean/\n",
      "Testing sets : test-clean/\n",
      "---------------------------------------\n",
      "Processing flac2wav...\n",
      "---------------------------------------\n",
      "Processing wav2logfbank...\n",
      "---------------------------------------\n",
      "Preparing Training Dataset...\n",
      "['/home/sanskriti/Documents/GitHub/Listen-Attend-and-Spell-Pytorch/LibriSpeech/train-clean-100/103/1240/103-1240-0000.fb40.npy', '/home/sanskriti/Documents/GitHub/Listen-Attend-and-Spell-Pytorch/LibriSpeech/train-clean-100/103/1240/103-1240-0001.fb40.npy', '/home/sanskriti/Documents/GitHub/Listen-Attend-and-Spell-Pytorch/LibriSpeech/train-clean-100/103/1240/103-1240-0002.fb40.npy', '/home/sanskriti/Documents/GitHub/Listen-Attend-and-Spell-Pytorch/LibriSpeech/train-clean-100/103/1240/103-1240-0003.fb40.npy', '/home/sanskriti/Documents/GitHub/Listen-Attend-and-Spell-Pytorch/LibriSpeech/train-clean-100/103/1240/103-1240-0004.fb40.npy']\n",
      "['CHAPTER ONE MISSUS RACHEL LYNDE IS SURPRISED MISSUS RACHEL LYNDE LIVED JUST WHERE THE AVONLEA MAIN ROAD DIPPED DOWN INTO A LITTLE HOLLOW FRINGED WITH ALDERS AND LADIES EARDROPS AND TRAVERSED BY A BROOK', \"THAT HAD ITS SOURCE AWAY BACK IN THE WOODS OF THE OLD CUTHBERT PLACE IT WAS REPUTED TO BE AN INTRICATE HEADLONG BROOK IN ITS EARLIER COURSE THROUGH THOSE WOODS WITH DARK SECRETS OF POOL AND CASCADE BUT BY THE TIME IT REACHED LYNDE'S HOLLOW IT WAS A QUIET WELL CONDUCTED LITTLE STREAM\", \"FOR NOT EVEN A BROOK COULD RUN PAST MISSUS RACHEL LYNDE'S DOOR WITHOUT DUE REGARD FOR DECENCY AND DECORUM IT PROBABLY WAS CONSCIOUS THAT MISSUS RACHEL WAS SITTING AT HER WINDOW KEEPING A SHARP EYE ON EVERYTHING THAT PASSED FROM BROOKS AND CHILDREN UP\", \"AND THAT IF SHE NOTICED ANYTHING ODD OR OUT OF PLACE SHE WOULD NEVER REST UNTIL SHE HAD FERRETED OUT THE WHYS AND WHEREFORES THEREOF THERE ARE PLENTY OF PEOPLE IN AVONLEA AND OUT OF IT WHO CAN ATTEND CLOSELY TO THEIR NEIGHBOR'S BUSINESS BY DINT OF NEGLECTING THEIR OWN\", 'BUT MISSUS RACHEL LYNDE WAS ONE OF THOSE CAPABLE CREATURES WHO CAN MANAGE THEIR OWN CONCERNS AND THOSE OF OTHER FOLKS INTO THE BARGAIN SHE WAS A NOTABLE HOUSEWIFE HER WORK WAS ALWAYS DONE AND WELL DONE SHE RAN THE SEWING CIRCLE']\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "from pydub import AudioSegment\n",
    "import os\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from joblib import Parallel, delayed\n",
    "import scipy.io.wavfile as wav\n",
    "from python_speech_features import logfbank\n",
    "\n",
    "import argparse\n",
    "\n",
    "\n",
    "# parser = argparse.ArgumentParser(description='Librispeech preprocess.')\n",
    "\n",
    "# parser.add_argument('root', metavar='root', type=str,\n",
    "#                      help='Absolute file path to LibriSpeech. (e.g. /usr/downloads/LibriSpeech/)')\n",
    "\n",
    "# parser.add_argument('tr_sets', metavar='tr_sets', type=str, nargs='+',\n",
    "#                      help='Training datasets to process in LibriSpeech. (e.g. train-clean-100/)')\n",
    "\n",
    "# parser.add_argument('--dev_sets', metavar='dev_sets', type=str, nargs='+', default=[] ,\n",
    "#                      help='Validation datasets to process in LibriSpeech. (e.g. dev-clean/)')\n",
    "\n",
    "# parser.add_argument('--tt_sets', metavar='tt_sets', type=str, nargs='+', default=[] ,\n",
    "#                      help='Testing datasets to process in LibriSpeech. (e.g. test-clean/)')\n",
    "\n",
    "# parser.add_argument('--n_jobs', dest='n_jobs', action='store', default=-2 ,\n",
    "#                    help='number of cpu availible for preprocessing.\\n -1: use all cpu, -2: use all cpu but one')\n",
    "# parser.add_argument('--n_filters', dest='n_filters', action='store', default=40 ,\n",
    "#                    help='number of filters for fbank. (Default : 40)')\n",
    "# parser.add_argument('--win_size', dest='win_size', action='store', default=0.025 ,\n",
    "#                    help='window size during feature extraction (Default : 0.025 [25ms])')\n",
    "# parser.add_argument('--norm_x', dest='norm_x', action='store', default=False ,\n",
    "#                    help='Normalize features s.t. mean = 0 std = 1')\n",
    "\n",
    "\n",
    "# paras = parser.parse_args()\n",
    "\n",
    "# root = paras.root\n",
    "# train_path = paras.tr_sets\n",
    "# dev_path = paras.dev_sets\n",
    "# test_path = paras.tt_sets\n",
    "# n_jobs = paras.n_jobs\n",
    "# n_filters = paras.n_filters\n",
    "# win_size = paras.win_size\n",
    "# norm_x = paras.norm_x\n",
    "root = '/home/sanskriti/Documents/GitHub/Listen-Attend-and-Spell-Pytorch/LibriSpeech/'\n",
    "train_path = 'train-clean-100/'\n",
    "dev_path = 'dev-clean/'\n",
    "test_path = 'test-clean/'\n",
    "n_jobs = -2\n",
    "n_filters = 40\n",
    "win_size = 0.025\n",
    "norm_x = False\n",
    "\n",
    "\n",
    "def traverse(root,path,search_fix='.flac',return_label=False):\n",
    "    f_list = []\n",
    "\n",
    "    # for p in path:\n",
    "    p = path\n",
    "    p = root + p\n",
    "    for sub_p in sorted(os.listdir(p)):\n",
    "        for sub2_p in sorted(os.listdir(p+sub_p+'/')):\n",
    "            if return_label:\n",
    "                # Read trans txt\n",
    "                with open(p+sub_p+'/'+sub2_p+'/'+sub_p+'-'+sub2_p+'.trans.txt','r') as txt_file:\n",
    "                    for line in txt_file:\n",
    "                        f_list.append(' '.join(line[:-1].split(' ')[1:]))\n",
    "            else:\n",
    "                # Read acoustic feature\n",
    "                for file in sorted(os.listdir(p+sub_p+'/'+sub2_p)):\n",
    "                    if search_fix in file:\n",
    "                        file_path = p+sub_p+'/'+sub2_p+'/'+file\n",
    "                        f_list.append(file_path)   \n",
    "    return f_list\n",
    "\n",
    "def flac2wav(f_path):\n",
    "    flac_audio = AudioSegment.from_file(f_path, \"flac\")\n",
    "    flac_audio.export(f_path[:-5]+'.wav', format=\"wav\")\n",
    "\n",
    "def wav2logfbank(f_path):\n",
    "    (rate,sig) = wav.read(f_path)\n",
    "    fbank_feat = logfbank(sig,rate,winlen=win_size,nfilt=n_filters)\n",
    "    np.save(f_path[:-3]+'fb'+str(n_filters),fbank_feat)\n",
    "\n",
    "def norm(f_path,mean,std):\n",
    "    np.save(f_path,(np.load(f_path)-mean)/std)\n",
    "\n",
    "\n",
    "print('----------Processing Datasets----------')\n",
    "print('Training sets :',train_path)\n",
    "print('Validation sets :',dev_path)\n",
    "print('Testing sets :',test_path)\n",
    "\n",
    "\n",
    "\n",
    "# # flac2wav\n",
    "print('---------------------------------------')\n",
    "print('Processing flac2wav...',flush=True)\n",
    "\n",
    "# print('Training',flush=True)\n",
    "# tr_file_list = traverse(root,train_path)          \n",
    "# results = Parallel(n_jobs=n_jobs,backend=\"threading\")(delayed(flac2wav)(i) for i in tqdm(tr_file_list))\n",
    "\n",
    "# print('Validation',flush=True)\n",
    "# dev_file_list = traverse(root,dev_path)   \n",
    "# results = Parallel(n_jobs=n_jobs,backend=\"threading\")(delayed(flac2wav)(i) for i in tqdm(dev_file_list))\n",
    "\n",
    "# print('Testing',flush=True)\n",
    "# tt_file_list = traverse(root,test_path)   \n",
    "# results = Parallel(n_jobs=n_jobs,backend=\"threading\")(delayed(flac2wav)(i) for i in tqdm(tt_file_list))\n",
    "                  \n",
    "\n",
    "\n",
    "# # wav 2 log-mel fbank\n",
    "print('---------------------------------------')\n",
    "print('Processing wav2logfbank...',flush=True)\n",
    "\n",
    "# print('Training',flush=True)\n",
    "# results = Parallel(n_jobs=n_jobs,backend=\"threading\")(delayed(wav2logfbank)(i[:-4]+'wav') for i in tqdm(tr_file_list))\n",
    "\n",
    "# print('Validation',flush=True)\n",
    "# results = Parallel(n_jobs=n_jobs,backend=\"threading\")(delayed(wav2logfbank)(i[:-4]+'wav') for i in tqdm(dev_file_list))\n",
    "\n",
    "# print('Testing',flush=True)\n",
    "# results = Parallel(n_jobs=n_jobs,backend=\"threading\")(delayed(wav2logfbank)(i[:-4]+'wav') for i in tqdm(tt_file_list))\n",
    "                    \n",
    "\n",
    "\n",
    "# # log-mel fbank 2 feature\n",
    "print('---------------------------------------') \n",
    "print('Preparing Training Dataset...',flush=True)\n",
    "\n",
    "tr_file_list = traverse(root,train_path,search_fix='.fb'+str(n_filters))\n",
    "print(tr_file_list[:5])\n",
    "tr_text = traverse(root,train_path,return_label=True)\n",
    "print(tr_text[:5])\n",
    "\n",
    "X = []\n",
    "for f in tr_file_list:\n",
    "    X.append(np.load(f, encoding='latin1', allow_pickle=True))\n",
    "print(\"hello1\")\n",
    "\n",
    "# Normalize X\n",
    "if norm_x:\n",
    "    mean_x = np.mean(np.concatenate(X,axis=0),axis=0)\n",
    "    std_x = np.std(np.concatenate(X,axis=0),axis=0)\n",
    "\n",
    "    results = Parallel(n_jobs=n_jobs,backend=\"threading\")(delayed(norm)(i,mean_x,std_x) for i in tqdm(tr_file_list))\n",
    "print(\"hello2\")\n",
    "\n",
    "\n",
    "# Sort data by signal length (long to short)\n",
    "audio_len = [len(x) for x in X]\n",
    "print(\"hello3\")\n",
    "\n",
    "tr_file_list = [tr_file_list[idx] for idx in reversed(np.argsort(audio_len))]\n",
    "tr_text = [tr_text[idx] for idx in reversed(np.argsort(audio_len))]\n",
    "\n",
    "# Create char mapping\n",
    "char_map = {}\n",
    "char_map['<sos>'] = 0\n",
    "char_map['<eos>'] = 1\n",
    "char_idx = 2\n",
    "\n",
    "# map char to index\n",
    "for text in tr_text:\n",
    "    for char in text:\n",
    "        if char not in char_map:\n",
    "            char_map[char] = char_idx\n",
    "            char_idx +=1\n",
    "            \n",
    "# Reverse mapping\n",
    "rev_char_map = {v:k for k,v in char_map.items()}\n",
    "\n",
    "# Save mapping\n",
    "with open(root+'idx2chap.csv','w') as f:\n",
    "    f.write('idx,char\\n')\n",
    "    for i in range(len(rev_char_map)):\n",
    "        f.write(str(i)+','+rev_char_map[i]+'\\n')\n",
    "\n",
    "# text to index sequence\n",
    "tmp_list = []\n",
    "for text in tr_text:\n",
    "    tmp = []\n",
    "    for char in text:\n",
    "        tmp.append(char_map[char])\n",
    "    tmp_list.append(tmp)\n",
    "tr_text = tmp_list\n",
    "del tmp_list\n",
    "\n",
    "# write dataset\n",
    "file_name = 'train.csv'\n",
    "\n",
    "print('Writing dataset to '+root+file_name+'...',flush=True)\n",
    "\n",
    "with open(root+file_name,'w') as f:\n",
    "    f.write('idx,input,label\\n')\n",
    "    for i in range(len(tr_file_list)):\n",
    "        f.write(str(i)+',')\n",
    "        f.write(tr_file_list[i]+',')\n",
    "        for char in tr_text[i]:\n",
    "            f.write(' '+str(char))\n",
    "        f.write('\\n')\n",
    "\n",
    "print()\n",
    "print('Preparing Validation Dataset...',flush=True)\n",
    "\n",
    "dev_file_list = traverse(root,dev_path,search_fix='.fb'+str(n_filters))\n",
    "dev_text = traverse(root,dev_path,return_label=True)\n",
    "\n",
    "X = []\n",
    "for f in dev_file_list:\n",
    "    X.append(np.load(f))\n",
    "\n",
    "# Normalize X\n",
    "if norm_x:\n",
    "    results = Parallel(n_jobs=n_jobs,backend=\"threading\")(delayed(norm)(i,mean_x,std_x) for i in tqdm(dev_file_list))\n",
    "\n",
    "\n",
    "# Sort data by signal length (long to short)\n",
    "audio_len = [len(x) for x in X]\n",
    "\n",
    "dev_file_list = [dev_file_list[idx] for idx in reversed(np.argsort(audio_len))]\n",
    "dev_text = [dev_text[idx] for idx in reversed(np.argsort(audio_len))]\n",
    "\n",
    "# text to index sequence\n",
    "tmp_list = []\n",
    "for text in dev_text:\n",
    "    tmp = []\n",
    "    for char in text:\n",
    "        tmp.append(char_map[char])\n",
    "    tmp_list.append(tmp)\n",
    "dev_text = tmp_list\n",
    "del tmp_list\n",
    "\n",
    "# write dataset\n",
    "file_name = 'dev.csv'\n",
    "\n",
    "print('Writing dataset to '+root+file_name+'...',flush=True)\n",
    "\n",
    "with open(root+file_name,'w') as f:\n",
    "    f.write('idx,input,label\\n')\n",
    "    for i in range(len(dev_file_list)):\n",
    "        f.write(str(i)+',')\n",
    "        f.write(dev_file_list[i]+',')\n",
    "        for char in dev_text[i]:\n",
    "            f.write(' '+str(char))\n",
    "        f.write('\\n')\n",
    "\n",
    "print()\n",
    "print('Preparing Testing Dataset...',flush=True)\n",
    "\n",
    "test_file_list = traverse(root,test_path,search_fix='.fb'+str(n_filters))\n",
    "tt_text = traverse(root,test_path,return_label=True)\n",
    "\n",
    "X = []\n",
    "for f in test_file_list:\n",
    "    X.append(np.load(f))\n",
    "\n",
    "# Normalize X\n",
    "if norm_x:\n",
    "    results = Parallel(n_jobs=n_jobs,backend=\"threading\")(delayed(norm)(i,mean_x,std_x) for i in tqdm(test_file_list))\n",
    "\n",
    "\n",
    "# Sort data by signal length (long to short)\n",
    "audio_len = [len(x) for x in X]\n",
    "\n",
    "test_file_list = [test_file_list[idx] for idx in reversed(np.argsort(audio_len))]\n",
    "tt_text = [tt_text[idx] for idx in reversed(np.argsort(audio_len))]\n",
    "\n",
    "# text to index sequence\n",
    "tmp_list = []\n",
    "for text in tt_text:\n",
    "    tmp = []\n",
    "    for char in text:\n",
    "        tmp.append(char_map[char])\n",
    "    tmp_list.append(tmp)\n",
    "tt_text = tmp_list\n",
    "del tmp_list\n",
    "\n",
    "# write dataset\n",
    "file_name = 'test.csv'\n",
    "\n",
    "print('Writing dataset to '+root+file_name+'...',flush=True)\n",
    "\n",
    "with open(root+file_name,'w') as f:\n",
    "    f.write('idx,input,label\\n')\n",
    "    for i in range(len(test_file_list)):\n",
    "        f.write(str(i)+',')\n",
    "        f.write(test_file_list[i]+',')\n",
    "        for char in tt_text[i]:\n",
    "            f.write(' '+str(char))\n",
    "        f.write('\\n')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7 (default, Sep 16 2021, 13:09:58) \n[GCC 7.5.0]"
  },
  "vscode": {
   "interpreter": {
    "hash": "8139efdc4ccbd28e2239dc8ab38306d1c618d5b46a0f5cfc0b41fe6060f82dab"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
